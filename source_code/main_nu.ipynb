{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddfc77d",
   "metadata": {},
   "source": [
    "Functions:\n",
    "- Load/Extract new file\n",
    "-  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f86c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "# from pandas import ExcelWrite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee5804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1192a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\H'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\H'\n",
      "C:\\Users\\paxm\\AppData\\Local\\Temp\\ipykernel_11796\\3710805870.py:1: SyntaxWarning: invalid escape sequence '\\H'\n",
      "  master_file_directory = 'landing_zone\\2026\\2025-06\\Honda\\2026_Honda_Accessories-06-2025.xlsx'\n"
     ]
    }
   ],
   "source": [
    "master_file_directory = 'landing_zone\\2026\\2025-06\\Honda\\2026_Honda_Accessories-06-2025.xlsx'\n",
    "\n",
    "processes_files_storage_path ='C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/ready_to_upload'\n",
    "\n",
    "\n",
    "# masterfile = pd.read_excel( f'..\\landing_zone\\2026\\2025-06\\Honda\\2026_Honda_Accessories-06-2025.xlsx', engine='openpyxl')\n",
    "\n",
    "# masterfile = pd.read_excel(f'C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/2025-06/Honda/2026_Honda_Accessories-06-2025.xlsx', engine='openpyxl')\n",
    "\n",
    "\n",
    "# file_name ='2025-Honda-Master-June25'\n",
    "\n",
    "# master_file_path_25 = f'C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/2025-06/Honda/{file_name}.xlsx'\n",
    "\n",
    "common_columns = [\"Description\", \"Part Number\", \"List Price\", \"Labour\", \"Price\", \"Comments\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b584b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_path =f'C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/2025-06/Honda/2026-Honda_Accessories-06_2025.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc143d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a function that will read take in an excel file as an input, and find all the worskheets in the file and create a dataframe for each worksheet. This function will return a dictionary of dataframes with the worksheet names as the keys.\n",
    "def read_excel_sheets(file_path):\n",
    "    \"\"\"\n",
    "    Reads an Excel file and returns a dictionary of DataFrames for each sheet.\n",
    "    \n",
    "    :param file_path: Path to the Excel file.\n",
    "    :return: Dictionary with sheet names as keys and DataFrames as values.\n",
    "    \"\"\"\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    sheets_dict = {sheet_name: pd.read_excel(xls, sheet_name=sheet_name, engine='openpyxl') for sheet_name in xls.sheet_names}\n",
    "\n",
    "    # Optionally, you can close the Excel file after reading\n",
    "    # first check if the file is open and the sheet_dict is not empty\n",
    "    if sheets_dict and hasattr(xls, 'close'):\n",
    "        xls.close()\n",
    "    \n",
    "    return sheets_dict, xls.sheet_names\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7f4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_dict, sheets_names = read_excel_sheets(master_file_path_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0b8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to clean up the dataframes by, looping through each dataframe in the dictionary, check every column name and replace any \\n with a space from the names. and make sure that if the column name has a space at the end, it is removed. and ensure that their no 2 spaces in the column names.    \n",
    "\n",
    "\n",
    "def clean_dataframe_columns(df_dict):\n",
    "    \"\"\"\n",
    "    Cleans the column names of DataFrames in a dictionary by replacing newline characters with spaces.\n",
    "    \n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :return: Dictionary with cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    for key, df in df_dict.items():\n",
    "        df.columns = [col.replace('\\n', ' ') for col in df.columns]\n",
    "        df.columns = [col.replace('  ', ' ') for col in df.columns]\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "        df_dict[key] = df\n",
    "    return df_dict\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e4a3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_df_dict =clean_dataframe_columns(master_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe86516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes in a make, year, and model name and returns a vehicle model number. The function will check if the model name is in the model_mapping dictionary, if yes, it will return the model number, if not, it will return an empty string.\n",
    "\n",
    "# Placeholder for database search logic\n",
    "\n",
    "model_mapping = {\n",
    "        'Honda': {'2026': \n",
    "                        {'CRV': \n",
    "                            {'LX': '2026-Honda-LX', 'Sport': '2026-Honda-Sport', 'Trailsport': '2026-Honda-Trailsport', 'EX-L': '2026-Honda-EX-L', 'Touring': '2026-Honda-Touring', 'Sport-Hybrid':'2026-Honda-Sport-Hybrid'}, \n",
    "                        'Civic': {'LX': '2026-Honda-LX', 'Sport': '2026-Honda-Sport'}\n",
    "                        },\n",
    "                  '2025': \n",
    "                        {'CRV': \n",
    "                            {'LX': '2025-Honda-LX', 'Sport': '2025-Honda-Sport', 'Trailsport': '2025-Honda-Trailsport', 'EX-L': '2025-Honda-EX-L', 'Touring': '2025-Honda-Touring', 'Sport-Hybrid':'2025-Honda-Sport-Hybrid'}, \n",
    "                        'Civic': {'LX': '2025-Honda-LX', 'Sport': '2025-Honda-Sport'}\n",
    "                        },\n",
    "                  '2024': \n",
    "                        {'CRV': \n",
    "                            {'LX': '2024-Honda-LX', 'Sport': '2024-Honda-Sport', 'Trailsport': '2024-Honda-Trailsport', 'EX-L': '2024-Honda-EX-L', 'Touring': '2024-Honda-Touring', 'Sport-Hybrid':'2024-Honda-Sport-Hybrid'}, \n",
    "                        'Civic': {'LX': '2024-Honda-LX', 'Sport': '2024-Honda-Sport'}\n",
    "                        },\n",
    "                   },\n",
    "        'Toyota': {'2026': {'RAV4':{'RAV4': '2026-Toyota-RAV4'}, 'Camry': {'Camry': '2026-Toyota-Camry'}}}\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def get_vehicle_model_number(make, year, trim, model_name):\n",
    "    \"\"\"\n",
    "    Returns the vehicle model number based on make, year, and model name.\n",
    "    \n",
    "    :param make: Make of the vehicle.\n",
    "    :param year: Year of the vehicle.\n",
    "    :param model_name: Name of the vehicle model.\n",
    "    :param trim: Trim of the vehicle model.\n",
    "    :return: Vehicle model number or empty string if not found.\n",
    "    \"\"\"\n",
    "    model_num = model_mapping.get(make, {}).get(year, {}).get(model_name, {}).get(trim, '')\n",
    "\n",
    "    if model_num == '':\n",
    "        return False\n",
    "    \n",
    "    return model_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b5218fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vehicle_model_number(make='Honda', year='2024', trim='CRV', model_name='Sport-Hybrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb835b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  takes in 1 Df and create dfs for each of the model trims of that model\n",
    "#  Returns a dictionary with all the trim-dfs (values) under the model name as key\n",
    "\n",
    "def create_unique_trim_dfs_by_model(loaded_df, make, year, model_name):\n",
    "\n",
    "    # check if df has content\n",
    "\n",
    "    if loaded_df is not None:\n",
    "        unique_model_dfs_dict = {}\n",
    "\n",
    "        for column in loaded_df.columns:\n",
    "            # Find the specific model columns\n",
    "            if column not in common_columns and pd.notnull(loaded_df[column]).any():\n",
    "                new_df = loaded_df[[column] + common_columns].copy() # create df for each model ono the sheet\n",
    "            \n",
    "\n",
    "                # drop all records with N/A or empty \n",
    "                new_df = new_df[new_df[column].notnull() & (new_df[column] != '')]\n",
    "                               \n",
    "                # Add year column and assign value\n",
    "                new_df.insert (0,\"Year\", year)\n",
    "\n",
    "                trim_name = column  # Set trim_name\n",
    "\n",
    "                # Fetching Vehicle model\n",
    "                model_num = get_vehicle_model_number(make=make, year=year, trim=trim_name, model_name = model_name)\n",
    "                \n",
    "\n",
    "                new_df.rename(columns={column:'Model'}, inplace=True)\n",
    "\n",
    "                if model_num != False:        \n",
    "                    new_df['Model'] = model_num\n",
    "                else:\n",
    "                    new_df['Model_Temp'] = model_name + '-' + trim_name + '-' + year\n",
    "\n",
    "                # Change column names to standard Rate Importer columns\n",
    "                if 'FRT' in new_df.columns:\n",
    "                    new_df.rename(columns={'FRT':'Hours'}, inplace=True)\n",
    "                if 'List Price' in new_df.columns:\n",
    "                    new_df.rename(columns={'List Price':'Cost'}, inplace=True)\n",
    "                if 'Installed Price' in new_df.columns:\n",
    "                    new_df.rename(columns={'Installed Price':'Price'}, inplace=True)\n",
    "                if 'Part Number' in new_df.columns:\n",
    "                    new_df.rename(columns={'Part Number':'Part'}, inplace=True)\n",
    "                if 'Dealer Net' in new_df.columns:\n",
    "                    new_df.drop(column={'Dealer Net'}, inplace= True)\n",
    "                \n",
    "                \n",
    "\n",
    "                if not new_df.empty:\n",
    "                    if \"unnamed\" not in column.lower():\n",
    "                        unique_model_dfs_dict[column] = new_df\n",
    "    \n",
    "    return unique_model_dfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c147bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the process to create dfs for each models 1 by 1\n",
    "\n",
    "def create_all_unique_model_df(master_df_dict, make, year):\n",
    "    all_model_df_dict = {}\n",
    "\n",
    "    for model in master_df_dict.keys():\n",
    "        clean_model_name = model.split('-')[0]\n",
    "        all_model_df_dict[clean_model_name]= create_unique_trim_dfs_by_model(master_df_dict.get(model), make = make, year=year, model_name=clean_model_name) \n",
    "        \n",
    "    return all_model_df_dict \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e28a6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = create_all_unique_model_df(master_df_dict, make='Honda', year='2025')\n",
    "\n",
    "\n",
    "# res.get(\"CRV\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea20b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.get(\"CRV\").get(\"Sport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a13107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes in a dictionary of dataframes and saves it as an excel sheet with each dataframe as a separate sheet. The function will take in the dictionary of dataframes and a file path to save the excel file. The function will return nothing. The function will use the pandas ExcelWriter to write the dataframes to the excel file. The function will also check if the file path exists, if not, it will create the directory. The function will also check if the file already exists, if yes, it will overwrite the file. I want to modify this function, the input will be a dictionary of a dataframes, a directory path, the year, and make. Withe provided directory path, th goal is to find the country folder first, say 2026, if it doesn't exist create it, and then find the make, it doesn't exist create it based on the provided string, and then dave the fail in the format of \"YYYY-Make_Accessories-UploadFile-timestamp with seconds.xlsx\". The function will also ensure that the directory exists before saving the file. \n",
    "\n",
    "def save_dfs_to_excel(dict_all_dfs_by_model_ready, directory_path, year, make,lang):\n",
    "    \"\"\"\n",
    "    Saves a dictionary of DataFrames to an Excel file with each DataFrame as a separate sheet.\n",
    "    \n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :param directory_path: Directory path to save the Excel file.\n",
    "    :param year: Year to be included in the file name.\n",
    "    :param make: Make to be included in the file name.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the directory structure if it doesn't exist\n",
    "    year_dir = os.path.join(directory_path, str(year))\n",
    "    make_dir = os.path.join(year_dir, make)\n",
    "    \n",
    "    os.makedirs(make_dir, exist_ok=True)\n",
    "\n",
    "    # Create the file name with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    file_name = f\"{year}-{make}_Accessories-UploadFile_{lang}-{timestamp}.xlsx\"\n",
    "    file_path = os.path.join(make_dir, file_name)\n",
    "\n",
    "    # Save the DataFrames to an Excel file\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "\n",
    "        combined_sheets_df_by_model = {}\n",
    "\n",
    "            # First pass: write combined sheets\n",
    "        for model, trims_dict in dict_all_dfs_by_model_ready.items():\n",
    "            combined_df = pd.DataFrame()\n",
    "\n",
    "            for i, (trim, df) in enumerate(trims_dict.items()):\n",
    "                combined_df = pd.concat([combined_df, df if i == 0 else df.iloc[:]], ignore_index=True)\n",
    "\n",
    "            combined_sheet_name = f\"{model}- Combined\"\n",
    "            combined_df.to_excel(writer, sheet_name=combined_sheet_name, index=False)\n",
    "\n",
    "            # Store for later use\n",
    "            combined_sheets_df_by_model[model] = combined_df\n",
    "\n",
    "        # Second pass: write individual sheets\n",
    "        for model, trims_dict in dict_all_dfs_by_model_ready.items():\n",
    "            for trim, df in trims_dict.items():\n",
    "                sheet_name = f\"{model}-{trim}\"\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        # Access the workbook and apply green tab color to combined sheets\n",
    "        workbook = writer.book\n",
    "        for sheet_name in workbook.sheetnames:\n",
    "            if \"Combined\" in sheet_name:\n",
    "                workbook[sheet_name].sheet_properties.tabColor = \"00FF00\"  # Bright green\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df7e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e646fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_master_file_path, destination_path, make, year, lang):\n",
    "    \n",
    "    # Reading the master file\n",
    "    master_dict_dfs, sheets_names = read_excel_sheets(input_master_file_path)\n",
    "\n",
    "    # Clean dfs\n",
    "    cleaned_dfs_dic = clean_dataframe_columns(master_dict_dfs)\n",
    "\n",
    "    # Create unique df for each model\n",
    "    all_model_df_dict = create_all_unique_model_df(cleaned_dfs_dic, make, year)\n",
    "\n",
    "    # Save all dfs to excel \n",
    "    save_dfs_to_excel(all_model_df_dict, destination_path, year=year, make=make, lang=lang)\n",
    "    \n",
    "    print(f\"Yeaaah!!!!! Done. Your {lang} file is ready to upload. Check your Directory :)\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "027afa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '2025-Honda-Master-June25'\n",
    "input_master_file_path = f'C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/2025-06/Honda/{file_name}.xlsx'\n",
    "processes_files_storage_path ='C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/ready_to_upload'\n",
    "# file_name ='2025-Honda-Master-June25'\n",
    "\n",
    "# master_file_path_25 = f'C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/2025-06/Honda/{file_name}.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9567c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paxm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\openpyxl\\workbook\\child.py:99: UserWarning: Title is more than 31 characters. Some applications may not be able to read the file\n",
      "  warnings.warn(\"Title is more than 31 characters. Some applications may not be able to read the file\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeaaah!!!!! Done. Your FR file is ready to upload. Check your Directory :)\n"
     ]
    }
   ],
   "source": [
    "make= \"Honda\"\n",
    "input_year = '2026'\n",
    "\n",
    "Lang = 'FR'\n",
    "\n",
    "file_name = 'Honda_ACCY_Master_2026_FR'\n",
    "\n",
    "input_master_file_path = f'C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/Honda/2025-09/{file_name}.xlsx'\n",
    "\n",
    "main(input_master_file_path= input_master_file_path, destination_path=processes_files_storage_path, make=make, year=input_year, lang =Lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a utility that loads a table of vehicle profiles into a csv. This utility will do regular checks to see if there are any new vehicles added to the table and if so, it will update the csv file with the new vehicles. TO make updates, after getting the response from the database, it will check if there is any difference in teh already existing values, in csv catalogue and will update the csv accordingly. Let's say a particular record has had some changes, we'll then do it that way. Make sure that all data are in lower case to make sure that there are no duplicates because of case sensitivity. I want you to create a function, and do not worry about the query, presented as if the query has been emplimentated, and a path where to store the csv will be provided, and this where it will always be.  \n",
    "\n",
    "def update_vehicle_catalogue(csv_path, new_data):\n",
    "    \"\"\"\n",
    "    Updates the vehicle catalogue CSV with new data from the database.\n",
    "    \n",
    "    :param csv_path: Path to the vehicle catalogue CSV file.\n",
    "    :param new_data: New data from the database as a DataFrame.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Load existing catalogue if it exists\n",
    "    if os.path.exists(csv_path):\n",
    "        existing_catalogue = pd.read_csv(csv_path)\n",
    "        existing_catalogue = existing_catalogue.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    else:\n",
    "        existing_catalogue = pd.DataFrame()\n",
    "\n",
    "    # Convert new data to lower case for consistency\n",
    "    new_data = new_data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # Identify new or updated records\n",
    "    if not existing_catalogue.empty:\n",
    "        combined = pd.concat([existing_catalogue, new_data]).drop_duplicates(keep=False)\n",
    "    else:\n",
    "        combined = new_data\n",
    "\n",
    "    # If there are changes, update the CSV\n",
    "    if not combined.empty:\n",
    "        updated_catalogue = pd.concat([existing_catalogue, combined]).drop_duplicates().reset_index(drop=True)\n",
    "        updated_catalogue.to_csv(csv_path, index=False)\n",
    "        print(f\"Vehicle catalogue updated at {csv_path}\")\n",
    "    else:\n",
    "        print(\"No updates to the vehicle catalogue.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
