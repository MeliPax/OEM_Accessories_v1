{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddfc77d",
   "metadata": {},
   "source": [
    "Functions:\n",
    "- Load/Extract new file\n",
    "-  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77f86c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1192a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\H'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\H'\n",
      "C:\\Users\\paxm\\AppData\\Local\\Temp\\ipykernel_37348\\2908687198.py:1: SyntaxWarning: invalid escape sequence '\\H'\n",
      "  master_file_directory = 'landing_zone\\2026\\2025-06\\Honda\\2026_Honda_Accessories-06-2025.xlsx'\n"
     ]
    }
   ],
   "source": [
    "master_file_directory = 'landing_zone\\2026\\2025-06\\Honda\\2026_Honda_Accessories-06-2025.xlsx'\n",
    "\n",
    "processes_files_storage_path ='C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/ready_to_upload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9db97fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# masterfile = pd.read_excel( f'..\\landing_zone\\2026\\2025-06\\Honda\\2026_Honda_Accessories-06-2025.xlsx', engine='openpyxl')\n",
    "\n",
    "masterfile = pd.read_excel(f'C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/2025-06/Honda/2026_Honda_Accessories-06-2025.xlsx', engine='openpyxl')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b584b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_file_path =f'C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/2025-06/Honda/2026-Honda_Accessories-06_2025.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cc143d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a function that will read take in an excel file as an input, and find all the worskheets in the file and create a dataframe for each worksheet. This function will return a dictionary of dataframes with the worksheet names as the keys.\n",
    "def read_excel_sheets(file_path):\n",
    "    \"\"\"\n",
    "    Reads an Excel file and returns a dictionary of DataFrames for each sheet.\n",
    "    \n",
    "    :param file_path: Path to the Excel file.\n",
    "    :return: Dictionary with sheet names as keys and DataFrames as values.\n",
    "    \"\"\"\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    sheets_dict = {sheet_name: pd.read_excel(xls, sheet_name=sheet_name, engine='openpyxl') for sheet_name in xls.sheet_names}\n",
    "\n",
    "    # Optionally, you can close the Excel file after reading\n",
    "    # first check if the file is open and the sheet_dict is not empty\n",
    "    if sheets_dict and hasattr(xls, 'close'):\n",
    "        xls.close()\n",
    "    return sheets_dict, xls.sheet_names\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ad0b8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to clean up the dataframes by, looping through each dataframe in the dictionary, check every column name and replace any \\n with a space from the names. and make sure that if the column name has a space at the end, it is removed. and ensure that their no 2 spaces in the column names.    \n",
    "\n",
    "\n",
    "def clean_dataframe_columns(df_dict):\n",
    "    \"\"\"\n",
    "    Cleans the column names of DataFrames in a dictionary by replacing newline characters with spaces.\n",
    "    \n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :return: Dictionary with cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    for key, df in df_dict.items():\n",
    "        df.columns = [col.replace('\\n', ' ') for col in df.columns]\n",
    "        df.columns = [col.replace('  ', ' ') for col in df.columns]\n",
    "        df.columns = [col.strip() for col in df.columns]\n",
    "        df_dict[key] = df\n",
    "    return df_dict\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfce377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c5e9e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "common_columns = [\"Description\", \"Part Number\", \"List Price\", \"FRT\", \"Installed Price\", \"Comments\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "314a528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that will take a dic of dataframes, loop through the dataframes and will create other dataframes based on the following instructions: Find the common columns which are : Description, Part, Cost, Hours, Price, and Comments.. for all the remaining columns which are unique model columns, create a new dataframe with the names as the column name. The dataframes will be made of a unique model column and all the common columns. The new dataframe will be named as the column name of the unique model column. The function will return a dictionary of dataframes with the unique model column names as the keys and the new dataframes as the values. For each unique model to determine whether a row will be added to the df, check in the master df if the column is not null and the value is not empty. If the value is not null and not empty, then add the row to the new dataframe. If the value is null or empty, then do not add the row to the new dataframe. The function will also check if the column name is in the master df columns before creating the new dataframe. Also, for the value to be considered to be not empty, it should have a \"•\" in the cell. If the value does not have a \"•\", then it is considered empty and will not be added to the new dataframe. \n",
    "\n",
    "def create_unique_trim_dfs_old(master_df_dict):\n",
    "    \"\"\"\n",
    "    Creates DataFrames for each unique model column in the master DataFrame.\n",
    "    \n",
    "    :param master_df_dict: Dictionary of DataFrames with sheet names as keys.\n",
    "    :return: Dictionary of DataFrames with unique model column names as keys.\n",
    "    \"\"\"\n",
    "    unique_model_dfs_dict = {}\n",
    "\n",
    "    for model in master_df_dict.keys():\n",
    "        master_df = master_df_dict.get(model)\n",
    "        \n",
    "        if master_df is not None:\n",
    "\n",
    "            # Loop through each column in the master DataFrame\n",
    "            for column in master_df.columns:\n",
    "                if column not in common_columns and pd.notnull(master_df[column]).any():\n",
    "                        \n",
    "                    # # Create a new DataFrame for the unique model column\n",
    "                    new_df = master_df[[column] + common_columns].copy()\n",
    "                    print(new_df.head(1))                    \n",
    "                    # # Filter rows where the unique model column has a \"•\" and is not null or empty\n",
    "                    \n",
    "                    # # re-write the newdf population to check if the cell is empty or contains string \"N/A\", if yes, then do not add the data. otherwise, add the data \n",
    "                    # new_df = new_df[new_df[column].notnull() & (new_df[column] != '')] \n",
    "                \n",
    "                    # # If the new DataFrame is not empty, add it to the dictionary\n",
    "                    # if not new_df.empty:\n",
    "                    #     unique_model_dfs_dict[column] = new_df\n",
    "        \n",
    "    return unique_model_dfs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bb835b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_trim_dfs(master_df_dict):\n",
    "    \"\"\"\n",
    "    Creates DataFrames for each unique model column in the master DataFrame.\n",
    "    \n",
    "    :param master_df_dict: Dictionary of DataFrames with sheet names as keys and master sheet def/content as value.\n",
    "    :return: Dictionary of DataFrames with unique trims column names as keys and unique trim df as value.\n",
    "    \"\"\"\n",
    "    unique_model_dfs_dict = {}\n",
    "\n",
    "    # Loop through all the keys of the dict\n",
    "    # if the value is not empty, then break it down\n",
    "#     print(master_df_dict)\n",
    "\n",
    "\n",
    "    for model in master_df_dict.keys():\n",
    "        \n",
    "\n",
    "        master_df = master_df_dict.get(model)\n",
    "        \n",
    "        if master_df is not None:\n",
    "            \n",
    "            # Loop through each column in the master DataFrame\n",
    "            for column in master_df.columns:\n",
    "                if column not in common_columns and pd.notnull(master_df[column]).any():\n",
    "                    \n",
    "                    # # Create a new DataFrame for the unique model column\n",
    "                    new_df = master_df[[column] + common_columns].copy()\n",
    "                    \n",
    "                # Filter rows where the unique model column has a \"•\" and is not null or empty                    \n",
    "                # re-write the newdf population to check if the cell is empty or contains string \"N/A\", if yes, then do not add the data. otherwise, add the data \n",
    "                    new_df = new_df[new_df[column].notnull() & (new_df[column] != '')] \n",
    "                \n",
    "                    # If the new DataFrame is not empty, add it to the dictionary\n",
    "                    if not new_df.empty:\n",
    "                        unique_model_dfs_dict[model][column] = new_df\n",
    "    # unique_model_dfs_dict = master_df_dict  \n",
    "    return unique_model_dfs_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33aaa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes in a dictionary of dataframes and a string for the year and adds a column \"Year\" on each dataframe and add in the year value to the column. The function will return the dictionary of dataframes with the new column added. I want to add the new column year at the beginning of the dataframe. The year will be a string value, for example, \"2026\". The function will loop through each dataframe in the dictionary and add the column to each dataframe.\n",
    "\n",
    "def add_year_column(df_dict, year):\n",
    "    \"\"\"\n",
    "    Adds a 'Year' column to each DataFrame in the dictionary.\n",
    "    \n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :param year: Year to be added as a string.\n",
    "    :return: Dictionary with DataFrames having the 'Year' column added.\n",
    "    \"\"\"\n",
    "    for key, df in df_dict.items():\n",
    "        df.insert(0, 'Year', year)  # Insert 'Year' column at the beginning\n",
    "        df_dict[key] = df\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "\n",
    "# create a function that removed the year column from the dataframes in the dictionary. The function will loop through each dataframe in the dictionary and remove the column \"Year\" if it exists. The function will return the dictionary of dataframes with the column removed.\n",
    "def remove_year_column(df_dict):\n",
    "    \"\"\"\n",
    "    Removes the 'Year' column from each DataFrame in the dictionary if it exists.\n",
    "    \n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :return: Dictionary with DataFrames having the 'Year' column removed.\n",
    "    \"\"\"\n",
    "    for key, df in df_dict.items():\n",
    "        if 'Year' in df.columns:\n",
    "            df.drop(columns=['Year'], inplace=True)\n",
    "        df_dict[key] = df\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a13107ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes in a dictionary of dataframes and saves it as an excel sheet with each dataframe as a separate sheet. The function will take in the dictionary of dataframes and a file path to save the excel file. The function will return nothing. The function will use the pandas ExcelWriter to write the dataframes to the excel file. The function will also check if the file path exists, if not, it will create the directory. The function will also check if the file already exists, if yes, it will overwrite the file. I want to modify this function, the input will be a dictionary of a dataframes, a directory path, the year, and make. Withe provided directory path, th goal is to find the country folder first, say 2026, if it doesn't exist create it, and then find the make, it doesn't exist create it based on the provided string, and then dave the fail in the format of \"YYYY-Make_Accessories-UploadFile-timestamp with seconds.xlsx\". The function will also ensure that the directory exists before saving the file. \n",
    "\n",
    "def save_dfs_to_excel(df_dict, directory_path, year, make, trim):\n",
    "    \"\"\"\n",
    "    Saves a dictionary of DataFrames to an Excel file with each DataFrame as a separate sheet.\n",
    "    \n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :param directory_path: Directory path to save the Excel file.\n",
    "    :param year: Year to be included in the file name.\n",
    "    :param make: Make to be included in the file name.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Create the directory structure if it doesn't exist\n",
    "    year_dir = os.path.join(directory_path, str(year))\n",
    "    make_dir = os.path.join(year_dir, make)\n",
    "    \n",
    "    os.makedirs(make_dir, exist_ok=True)\n",
    "\n",
    "    # Create the file name with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    file_name = f\"{year}-{make}-{trim}_Accessories-UploadFile-{timestamp}.xlsx\"\n",
    "    file_path = os.path.join(make_dir, file_name)\n",
    "\n",
    "    # Save the DataFrames to an Excel file\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "        for sheet_name, df in df_dict.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    \n",
    "    print(f\"DataFrames saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7d20593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# save_dfs_to_excel(unique_model_dfs_dict, processes_files_storage_path, year='2026', make='Honda', trim='CRV')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d80aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that takes in a dictionary of dataframes, a string with manufacture name, and a string with the year and runs a database search to a database to get a model string of the vehicle. If the model value is not empty, then create a new column names Model and populate the model number into the whole column. \n",
    "\n",
    "\n",
    "def add_model_column(df_dict, make, year):\n",
    "    \"\"\"\n",
    "    Adds a 'Model' column to each DataFrame in the dictionary based on a database search.\n",
    "    \n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :param make: Make of the vehicle.\n",
    "    :param year: Year of the vehicle.\n",
    "    :return: Dictionary with DataFrames having the 'Model' column added.\n",
    "    \"\"\"\n",
    "    # Placeholder for database search logic\n",
    "    # For demonstration, we'll assume a simple mapping\n",
    "    model_mapping = {\n",
    "        'Honda': {'2026': 'CRV', '2025': 'Civic'},\n",
    "        'Toyota': {'2026': 'RAV4', '2025': 'Camry'}\n",
    "    }\n",
    "    \n",
    "    model = model_mapping.get(make, {}).get(year, '')\n",
    "\n",
    "    for key, df in df_dict.items():\n",
    "        if model:\n",
    "            df['Model'] = model\n",
    "        else:\n",
    "            df['Model'] = ''  # or handle as needed\n",
    "        df_dict[key] = df\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4195bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a utility that loads a table of vehicle profiles into a csv. This utility will do regular checks to see if there are any new vehicles added to the table and if so, it will update the csv file with the new vehicles. TO make updates, after getting the response from the database, it will check if there is any difference in teh already existing values, in csv catalogue and will update the csv accordingly. Let's say a particular record has had some changes, we'll then do it that way. Make sure that all data are in lower case to make sure that there are no duplicates because of case sensitivity. I want you to create a function, and do not worry about the query, prented as if the query has been emplimentated, and a path where to store the csv will be provided, and this where it will always be.  \n",
    "\n",
    "def update_vehicle_catalogue(csv_path, new_data):\n",
    "    \"\"\"\n",
    "    Updates the vehicle catalogue CSV with new data from the database.\n",
    "    \n",
    "    :param csv_path: Path to the vehicle catalogue CSV file.\n",
    "    :param new_data: New data from the database as a DataFrame.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Load existing catalogue if it exists\n",
    "    if os.path.exists(csv_path):\n",
    "        existing_catalogue = pd.read_csv(csv_path)\n",
    "        existing_catalogue = existing_catalogue.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    else:\n",
    "        existing_catalogue = pd.DataFrame()\n",
    "\n",
    "    # Convert new data to lower case for consistency\n",
    "    new_data = new_data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "    # Identify new or updated records\n",
    "    if not existing_catalogue.empty:\n",
    "        combined = pd.concat([existing_catalogue, new_data]).drop_duplicates(keep=False)\n",
    "    else:\n",
    "        combined = new_data\n",
    "\n",
    "    # If there are changes, update the CSV\n",
    "    if not combined.empty:\n",
    "        updated_catalogue = pd.concat([existing_catalogue, combined]).drop_duplicates().reset_index(drop=True)\n",
    "        updated_catalogue.to_csv(csv_path, index=False)\n",
    "        print(f\"Vehicle catalogue updated at {csv_path}\")\n",
    "    else:\n",
    "        print(\"No updates to the vehicle catalogue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f67210e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for database search logic\n",
    "\n",
    "model_mapping = {\n",
    "        'Honda': {'2026': \n",
    "                  {'CRV': \n",
    "                    {'LX': '2026-Honda-LX', 'Sport': '2026-Honda-Sport', 'Trailsport': '2026-Honda-Trailsport', 'EX-L': '2026-Honda-EX-L', 'Touring': '2026-Honda-Touring', 'Sport-Hybrid':'2026-Honda-Sport-Hybrid'}\n",
    "                    }, \n",
    "                   'Civic': {'LX': '2026-Honda-LX', 'Sport': '2026-Honda-Sport'},\n",
    "        'Toyota': {'2026': {'RAV4':{'RAV4': '2026-Toyota-RAV4'}, 'Camry': {'Camry': '2026-Toyota-Camry'}}}\n",
    "    }}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901a668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc667843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes in a make, year, and model name and returns a vehicle model number. The function will check if the model name is in the model_mapping dictionary, if yes, it will return the model number, if not, it will return an empty string.\n",
    "\n",
    "def get_vehicle_model_number(make, year, trim, model_name):\n",
    "    \"\"\"\n",
    "    Returns the vehicle model number based on make, year, and model name.\n",
    "    \n",
    "    :param make: Make of the vehicle.\n",
    "    :param year: Year of the vehicle.\n",
    "    :param model_name: Name of the vehicle model.\n",
    "    :param trim: Trim of the vehicle model.\n",
    "    :return: Vehicle model number or empty string if not found.\n",
    "    \"\"\"\n",
    "    return model_mapping.get(make, {}).get(year, {}).get(trim, {}).get(model_name, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b27ac319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2026-Honda-Sport-Hybrid'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vehicle_model_number(make='Honda', year='2026', trim='CRV', model_name='Sport-Hybrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e914d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to add a model column to the unique_model_dfs_dict dictionary. The function will take in a make, year, and model name and will call the get_vehicle_model_number function to get the model number. If the model number is not empty, it will add a new column named Model to each dataframe in the dictionary and populate it with the model number. If the model number is empty, it will not add the column.\n",
    "\n",
    "def add_model_column(df_dict, make, year, trim):\n",
    "    \"\"\"\n",
    "    Adds a 'Model' column to each DataFrame in the dictionary based on a database search.\n",
    "    Fetches model number using get_vehicle_model_number function.\n",
    "\n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :return: Dictionary with DataFrames having the 'Model' column added.\n",
    "    \"\"\"\n",
    "\n",
    "      # Example trim, can be modified as needed\n",
    "    \n",
    "    for key, df in df_dict.items():\n",
    "        model_name = key.split()[0]  # Assuming the model name is part of the key\n",
    "        model_number = get_vehicle_model_number(make=make, year=year, trim=trim, model_name=model_name)\n",
    "        \n",
    "        if model_number:\n",
    "            print(\"I am here\")\n",
    "            df.insert(1, 'Model', model_number)\n",
    "            df.drop(columns=[model_name], inplace=True, errors='ignore')  # Remove 'Description' if it exists\n",
    "        else:\n",
    "            df.insert(1, 'Model', '')  # or handle as needed\n",
    "        df_dict[key] = df\n",
    "    \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a764d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a drop model column function that will remove the model column from the dataframes in the dictionary. The function will loop through each dataframe in the dictionary and remove the column \"Model\" if it exists. The function will return the dictionary of dataframes with the column removed.\n",
    "\n",
    "def drop_model_column(df_dict): \n",
    "    \"\"\"\n",
    "    Removes the 'Model' column from each DataFrame in the dictionary if it exists.\n",
    "    \n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :return: Dictionary with DataFrames having the 'Model' column removed.\n",
    "    \"\"\"\n",
    "    for key, df in df_dict.items():\n",
    "        if 'Model' in df.columns:\n",
    "            df.drop(columns=['Model'], inplace=True)\n",
    "        df_dict[key] = df\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "abcefc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sheets(df_dict):\n",
    "    \"\"\"\n",
    "    Combines all DataFrames in the dictionary into a single DataFrame.\n",
    "    \n",
    "    :param df_dict: Dictionary of DataFrames.\n",
    "    :return: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    combined_df = pd.concat(df_dict.values(), ignore_index=True)\n",
    "\n",
    "    print(f'Combined_df_columns: {combined_df['Model'].unique()}')\n",
    "    print(f'dict_keys: {df_dict.keys()}')\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc57f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5743716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a main function that will call all the functions in the correct order to process the master file and save the dataframes to an excel file. The function will take in the master file path, processes files storage path, year, and make as parameters. The function will return nothing.\n",
    "\n",
    "def main(master_file_path, processes_files_storage_path, year, make, trim):\n",
    "\n",
    "    # Step 1: Read the master file and create a dictionary of DataFrames\n",
    "    master_df_dict, sheet_names = read_excel_sheets(master_file_path)\n",
    "    print(f\"Master file read successfully with {len(master_df_dict)} sheets.\")\n",
    "    \n",
    "    # Step 2: Clean the DataFrame columns\n",
    "    master_df_dict = clean_dataframe_columns(master_df_dict)\n",
    "\n",
    "\n",
    "    # for model_master_file in master_df_dict.keys():\n",
    "        \n",
    "    # Step 3: Create unique model DataFrames\n",
    "\n",
    "    unique_model_dfs_dict = create_unique_trim_dfs(master_df_dict)\n",
    "    print (f\"Unique model DataFrames created with {len(unique_model_dfs_dict)} models.\")\n",
    "\n",
    "    # # Step 4: Add year column to unique model DataFrames\n",
    "    # unique_model_dfs_dict = add_year_column(unique_model_dfs_dict, year)\n",
    "    # print(f\"Year column added to unique model DataFrames for year {year}.\")\n",
    "    \n",
    "    # # Step 5: Add model column to unique model DataFrames\n",
    "    # unique_model_dfs_dict = add_model_column(df_dict= unique_model_dfs_dict, make= make, year=year, trim=trim)\n",
    "    # # print(f\"Model column added to unique model DataFrames for make {make} and year {year}.\")\n",
    "\n",
    "    # combined_dataframes = combine_sheets(unique_model_dfs_dict)\n",
    "    # unique_model_dfs_dict[f'{make}_{trim}_combined_sheets'] = combined_dataframes\n",
    "\n",
    "    # # Step 6: Save the DataFrames to an Excel file\n",
    "    # save_dfs_to_excel(unique_model_dfs_dict, processes_files_storage_path, year, make, trim)\n",
    "    \n",
    "    \n",
    "    # unique_model_dfs_dict = master_df_dict\n",
    "\n",
    "    return unique_model_dfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08af9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b5c35900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master file read successfully with 8 sheets.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'CivicHatch-Master'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m master_file_path = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/2025-06/Honda/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.xlsx\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# print(master_file_path)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m extract_df = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocesses_files_storage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2026\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHonda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrim\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCRV\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[131]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(master_file_path, processes_files_storage_path, year, make, trim)\u001b[39m\n\u001b[32m     10\u001b[39m master_df_dict = clean_dataframe_columns(master_df_dict)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# for model_master_file in master_df_dict.keys():\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Step 3: Create unique model DataFrames\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m unique_model_dfs_dict = \u001b[43mcreate_unique_trim_dfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_df_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# print (f\"Unique model DataFrames created with {len(unique_model_dfs_dict)} models.\")\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# # Step 4: Add year column to unique model DataFrames\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# unique_model_dfs_dict = master_df_dict\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unique_model_dfs_dict\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mcreate_unique_trim_dfs\u001b[39m\u001b[34m(master_df_dict)\u001b[39m\n\u001b[32m     33\u001b[39m                 \u001b[38;5;66;03m# If the new DataFrame is not empty, add it to the dictionary\u001b[39;00m\n\u001b[32m     34\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_df.empty:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m                     \u001b[43munique_model_dfs_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m[column] = new_df\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# unique_model_dfs_dict = master_df_dict  \u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unique_model_dfs_dict\n",
      "\u001b[31mKeyError\u001b[39m: 'CivicHatch-Master'"
     ]
    }
   ],
   "source": [
    "file_name ='2025-Honda-Master-June25'\n",
    "\n",
    "master_file_path = f'C:/Users/paxm/OneDrive - PBS SYSTEMS/Desktop/Office/Projects/OEM Accessory project/OEM Accessories_v1/landing_zone/2026/2025-06/Honda/{file_name}.xlsx'\n",
    "\n",
    "# print(master_file_path)\n",
    "extract_df = main(master_file_path, processes_files_storage_path, year='2026', make='Honda',trim='CRV')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8b68b3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CivicHatch-Master', 'HRV-Master', 'Pilot-Master', 'Passport-Master', 'ODYSSEY-Master', 'Ridgeline-Master', 'CRV-Master', 'Accord-Master'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "54aa43f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sport</th>\n",
       "      <th>TrailSport</th>\n",
       "      <th>Black Edition</th>\n",
       "      <th>Description</th>\n",
       "      <th>Part Number</th>\n",
       "      <th>List Price</th>\n",
       "      <th>FRT</th>\n",
       "      <th>Installed Price</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>Protection Package</td>\n",
       "      <td>24PAS-PRO-PK1 or\\n24PAS-PRO-PK2</td>\n",
       "      <td>$529.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>$561.40</td>\n",
       "      <td>24PAS-PRO-PK1 applies to Sport and Touring tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>Protection Package - Advance</td>\n",
       "      <td>24PAS-PRO-PKA1 or\\n24PAS-PRO-PKA2</td>\n",
       "      <td>$882.00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>$1,108.80</td>\n",
       "      <td>24PAS-PRO-PKA1 applies to Sport and Touring tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>Utility Package</td>\n",
       "      <td>22PAS-UTL-PK1</td>\n",
       "      <td>$2,985.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>$3,292.80</td>\n",
       "      <td>Inluding:\\nCrossbars\\nFender Flares\\nRunning B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>Cold Weather Package</td>\n",
       "      <td>19PAS-COLD-PK</td>\n",
       "      <td>$374.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>$552.50</td>\n",
       "      <td>5% Discount Applied. \\n\\nInclude: \\nEngine Blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tow Package, 3500 lbs</td>\n",
       "      <td>22PAS-TOW-1A or\\n22PAS-TOW-1B</td>\n",
       "      <td>$1,431.65</td>\n",
       "      <td>1.2</td>\n",
       "      <td>$1,626.05</td>\n",
       "      <td>5% Discount Applied.\\n\\n22PAS-TOW-1A includes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>NaN</td>\n",
       "      <td>•</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Diffused Sky Pearl (B-638P)</td>\n",
       "      <td>TUCH-B638P-A1</td>\n",
       "      <td>$21.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$21.50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>NaN</td>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>Radiant Red Metallic II (R-580M)</td>\n",
       "      <td>TUCHR580MA1</td>\n",
       "      <td>$21.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$21.50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>Fine Sanding Needles</td>\n",
       "      <td>TUCH-NEEDLES</td>\n",
       "      <td>$15.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$15.75</td>\n",
       "      <td>Quantity of 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>Fine Sanding Sticks</td>\n",
       "      <td>TUCH-STICKS</td>\n",
       "      <td>$4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$4.00</td>\n",
       "      <td>Quantity of 5.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>•</td>\n",
       "      <td>Paint Pen Replacement Tips</td>\n",
       "      <td>TUCH-TIPS</td>\n",
       "      <td>$23.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$23.00</td>\n",
       "      <td>Quantity of 25.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sport TrailSport Black Edition                       Description  \\\n",
       "0      •          •             •                Protection Package   \n",
       "1      •          •             •      Protection Package - Advance   \n",
       "2      •          •             •                   Utility Package   \n",
       "3      •          •             •              Cold Weather Package   \n",
       "4      •          •           NaN             Tow Package, 3500 lbs   \n",
       "..   ...        ...           ...                               ...   \n",
       "81   NaN          •           NaN       Diffused Sky Pearl (B-638P)   \n",
       "82   NaN          •             •  Radiant Red Metallic II (R-580M)   \n",
       "83     •          •             •              Fine Sanding Needles   \n",
       "84     •          •             •               Fine Sanding Sticks   \n",
       "85     •          •             •        Paint Pen Replacement Tips   \n",
       "\n",
       "                          Part Number List Price  FRT Installed Price  \\\n",
       "0     24PAS-PRO-PK1 or\\n24PAS-PRO-PK2    $529.00  0.2         $561.40   \n",
       "1   24PAS-PRO-PKA1 or\\n24PAS-PRO-PKA2    $882.00  1.4       $1,108.80   \n",
       "2                       22PAS-UTL-PK1  $2,985.00  1.9       $3,292.80   \n",
       "3                       19PAS-COLD-PK    $374.30  1.1         $552.50   \n",
       "4       22PAS-TOW-1A or\\n22PAS-TOW-1B  $1,431.65  1.2       $1,626.05   \n",
       "..                                ...        ...  ...             ...   \n",
       "81                      TUCH-B638P-A1     $21.50  0.0          $21.50   \n",
       "82                        TUCHR580MA1     $21.50  0.0          $21.50   \n",
       "83                       TUCH-NEEDLES     $15.75  0.0          $15.75   \n",
       "84                        TUCH-STICKS      $4.00  0.0           $4.00   \n",
       "85                          TUCH-TIPS     $23.00  0.0          $23.00   \n",
       "\n",
       "                                             Comments  \n",
       "0   24PAS-PRO-PK1 applies to Sport and Touring tri...  \n",
       "1   24PAS-PRO-PKA1 applies to Sport and Touring tr...  \n",
       "2   Inluding:\\nCrossbars\\nFender Flares\\nRunning B...  \n",
       "3   5% Discount Applied. \\n\\nInclude: \\nEngine Blo...  \n",
       "4   5% Discount Applied.\\n\\n22PAS-TOW-1A includes ...  \n",
       "..                                                ...  \n",
       "81                                                NaN  \n",
       "82                                                NaN  \n",
       "83                                     Quantity of 5.  \n",
       "84                                     Quantity of 5.  \n",
       "85                                    Quantity of 25.  \n",
       "\n",
       "[86 rows x 9 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_df.get('Passport-Master')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
